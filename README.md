# LIS706_Wisconsin-Homicide

The model will predict if a Firearm is used or not in committing a homicide in Wisconsin based on the data during the years 2000-2021. The dataset we chose to focus on is sourced from the Murder Accountability Project (MAP) and contains our country’s unsolved homicides. The entire data set is for the entire United States and the years 1976 through 2021. 
We plan to create models to predict the type of weapon used in a homicide based on variables like victim and offender demographics, location, and circumstances. With the focus on a type of weapon prediction model, then the “Weapon” variable would be a “class variable.” Also, based on the correlation map that we produced; it seems that a weapon’s choice is a very significant detail in murder prediction. 
Background information on the dataset:
The Murder Accountability Project is the most complete database of homicides in the United States currently available. The project was founded by Thomas K. Hargrove, a retired Washington, D.C., -based investigative journalist and former White House correspondent. Based on recent statistics, every year, around 5,000 killers get away with their crimes. Sadly, the rate at which police clear homicides through arrest has declined over the years until, today, nearly half of these homicides go unsolved. MAP is dedicated to educating people on the importance of accurately accounting for unsolved homicides within the United States. 
This dataset includes murders from the FBI's Supplementary Homicide Report from 1976 to 2021 and Freedom of Information Act data on more than 33,000 homicides that were not reported to the Justice Department. This dataset includes the age, race, sex, ethnicity of victims and perpetrators, in addition to the relationship between the victim and perpetrator and weapon used. 

Description 
In the original dataset, there are 31 variables and 849,144 cases that cover unsolved homicides from 1976 to 2021. Many of these variables will not be included in the final dataset for machine learning algorithms, as either being redundant or having too many missing values. Out of these instances, 250,790 cases are unsolved and have only the victims’ demographics and place and year when the homicide was perpetrated. So, after all the cleaning we ended up with 3657 rows. We will describe the clearing steps that have been followed later in the paper.

This is a list of current variables and their short explanations in the dataset:
ID (unique record identifier generated by MAP), 
CNTYFIPS (state and county of the reporting law enforcement agency), 
Ori (originating Agency making the report), 
State (state of the Originating Agency making the report), 
Agency (name of the law enforcement agency making the report), 
Agencytype (type of law enforcement agency making the report), 
Source (if record provided by FBI or was obtained by MAP under the Freedom of Information Act), 
Solved (if the homicide is solved now), 
Year (Year of homicide), 
StateName (the state of the reporting agency), 
Month (Month name),
Incident (a three-digit number describing the case number within the month in which a homicide occurred),
ActionType (the nature of the report received), 
Homicide (type of homicide), 
Situation (defining whether the crime had a single victim or multiple victims and whether there was a single offender, multiple offenders or the number of offenders was unknown), 
VicAge (victim’s age), VicSex (victim's gender), VicRace, VicEthinicy, 
OffAge (offender’s age), OffSex (offender’s gender), OffRace, OffEthnicity, 
Weapon (the weapon used in the crime), 
Relationship (relationship between the victim and the offender, if any), 
Circumstance (the circumstances (or theory) for the crime), 
Subcircum (several conditions in which the victim is reported to have been a criminal offender), VicCount (the number of additional victims (not counting the victim included in the current record)), 
OffCount (The number of additional offenders (not counting the offender included in the current record)), 
FileDate (the date a record was reported, not the date of the occurrence of the crime), 
MSA (the Metropolitan Statistical Area from which a record was reported).

What are we removing and why?
Some of the attributes are derived from each other, like “MSA”, “Ori”. Also, “Incident” variable describes the case number within the month in which a homicide occurred. “Situation” variable is another derived variable, combining the offender and victim count (“VicCount” and “OffCount”). 
Certain variables will not be helpful in predicting the crime hotspots, like “ActionType”, “ID”, “Source”, “FileDate”, “Subcircum”, “VicCount”, and “OffCount”. We transformed the names and year dates from character data type into integer. We will also not be going to use an Ethnicity indicator, as it is often empty.
What problems do we see in our data?
The original dataset itself is vast with 849,144 rows of data spanning from 1976 to 2021. So, the first thing we did was to select only the most recent years to reduce the dataset to a more manageable size. Using only the records from Wisconsin brings rows down to 4729 and then removing the 999s begins the total rows of data to 3657.  Years selected are 2000-2021.
Also, if a victim or a perpetrator were unidentifiable, the age, sex, race, and ethnic origin values would be missing. The later variable has mostly null values as is. Variable “Subcircum” has less than 4% of values present, so the best option is to just skip this variable in the final dataset. Variable “StateName” also has only 5% of values present and it is very similar to variables “State” and “CNTYFIPS” to keep it. 
Two variables “VicCount” and “OffCount” can be useful in identifying gangs for example, but they may not be as useful in trying to detect murder weapons. Most common value in these columns is zero, as there are no additional victims or offenders besides the ones described in the other columns.

Cleaning the dataset
Here is the outline of how we manipulated the dataset to put it into the state ready for modeling:
Split county and state into two columns so that we could pull only Wisconsin data. 
Limited the dataset to only records from Wisconsin. Which reduced the dataset from 849,144 rows to 4729 rows of data. 
We identified the outlier age of 999 (unidentified age) in the age columns. We looked into averaging the remaining ages and then replacing 999 with the average age, however, there are so many instances of unknown age values, that replacing these values with average age based on the county actually skews the data. In the end, we decided to completely drop rows of data that included the missing  (999) values.
Binned the ages because as individual data points they were not as useful and difficult to visualize in a meaningful way.  We chose to make more bins in certain age groups (20-60) because that is where the most data exists.  There were less data points in the 0-18 ages and 61-99.
After running some of the analysis, we realized that the bins of age groups should be mapped to single category names.  0-18=1, 19-25=2, 26-30=3, 31-35=4, 36-45=5, 46-60=6, 61-99=7.
The values for several attributes were converted from words to single letters.  The attributes altered were “OffSex” and “VicSex” and the values of “Male”, “Female”, and “Unknown” were changed into “M”, “F”, and “U”.
The values for race were changed as well: “A” = “Asian or Pacific Islander” or “B” = “Black” or “I” = “American Indian or Alaskan Native” or “W” = “White” or “U” = “Unknown” race.
We converted a “Month” column from the month name into a corresponding number.
We also Converted  “Weapon”, “Homicide”, “Solved”, etc columns into a categorical format (Nominal boolean type): Nominal attributes can be converted to real values. This is done by creating one new binary attribute for each category. For a given instance that has a category for that value, the binary attribute is set to 1 and the binary attributes for the other categories is set to 0. This process is called creating dummy variables.
After running the first couple of models, we realized that “Relationship” and “Circumstance” columns are not as important as other factors, and we chose to remove them to make a more agile model.
